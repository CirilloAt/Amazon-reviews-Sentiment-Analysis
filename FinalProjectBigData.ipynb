{"nbformat":4,"nbformat_minor":0,"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Classification","notebookOrigID":3966539020232142,"widgets":{}},"colab":{"name":"COPIAProfessoreClassification_datasetAMAZON.ipynb","provenance":[{"file_id":"https://github.com/gtolomei/big-data-computing/blob/d343ff5f7ed931dbb846e63ae3bbb7c30f9b3c63/notebooks/Classification.ipynb","timestamp":1623847430611}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ynbbqV4-2U3w"},"source":["## **Mount drive**"]},{"cell_type":"code","metadata":{"id":"7UvPPl1bIxcs"},"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9MI1nrwP8UK"},"source":["# **1. Description of the project**"]},{"cell_type":"markdown","metadata":{"id":"S_sUChCsP8w5"},"source":["The goal of our project is a *Binary Classification* that is able to predict the polarity of an Amazon Review. <br>We found a dataset available on Kaggle that is composed of three columns:\n","\n","*   ***Classification***: polarity of the review (2 for Positive, 1 for Negative)\n","*   ***Title*** of the review\n","*   ***Description*** of the review\n","\n","The Dataset is composed of about 3,6M of record and it is balanced (Positive reviews are more or less the same number).\n","We try to do our best using Colab Platform that is resource-limited."]},{"cell_type":"markdown","metadata":{"id":"P12FlWhXUV9g"},"source":["# **2. Setting up the Environment**"]},{"cell_type":"markdown","metadata":{"id":"651qM3Dsxchz"},"source":["## **Install Libraries**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0k-wgwX3CTB","executionInfo":{"status":"ok","timestamp":1624978150441,"user_tz":-120,"elapsed":3174,"user":{"displayName":"Atalla","photoUrl":"","userId":"01958578645143995824"}},"outputId":"f11747c3-7e19-42ab-a95c-e8c64b9f05da"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n","Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gkY8FpwzxhWT"},"source":["## **Import Libraries**"]},{"cell_type":"code","metadata":{"id":"Fh8zPg5APmYv"},"source":["import requests\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf\n","from pyspark import *\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.ml.classification import NaiveBayes\n","from pyspark.ml.classification import DecisionTreeClassifier\n","from pyspark.ml.classification import GBTClassifier\n","from pyspark.ml.classification import LinearSVC"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jO_aK_f-H1us"},"source":["## **Define utility functions**\n","In this section, we introduce two functions that we will use during this Colab.\n","One is for Text Cleaning and the other is for OneHotEncoding during Data Preparation.\n"]},{"cell_type":"code","metadata":{"id":"wpvTAkOHsO6B"},"source":["def clean_text(df, column_name):\n","    \"\"\" \n","    This function takes the raw text data and applies a standard NLP preprocessing pipeline consisting of the following steps:\n","      - Text cleaning\n","      - Tokenization\n","      - Stopwords removal\n","      - Stemming (Snowball stemmer)\n","\n","    parameter: dataframe\n","    returns: the input dataframe along with the `cleaned_content` column as the results of the NLP preprocessing pipeline\n","\n","    \"\"\"\n","    from pyspark.sql.functions import udf, col, lower, trim, regexp_replace\n","    from pyspark.ml.feature import Tokenizer, StopWordsRemover\n","    from nltk.stem.snowball import SnowballStemmer # BE SURE NLTK IS INSTALLED ON THE CLUSTER USING THE \"LIBRARIES\" TAB IN THE MENU\n","\n","    # Text preprocessing pipeline\n","    print(\"***** Text Preprocessing Pipeline *****\\n\")\n","\n","    # 1. Text cleaning\n","    print(\"# 1. Text Cleaning\\n\")\n","    # 1.a Case normalization\n","    print(\"1.a Case normalization:\")\n","    lower_case_news_df = df.select(lower(col(column_name)).alias(column_name))\n","    #lower_case_news_df.show(10)\n","    # 1.b Trimming\n","    print(\"1.b Trimming:\")\n","    trimmed_news_df = lower_case_news_df.select(trim(col(column_name)).alias(column_name))\n","    #trimmed_news_df.show(10)\n","    # 1.c Filter out punctuation symbols\n","    print(\"1.c Filter out punctuation:\")\n","    no_punct_news_df = trimmed_news_df.select((regexp_replace(col(column_name), \"[^a-zA-Z\\\\s]\", \"\")).alias(column_name))\n","    #no_punct_news_df.show(10)\n","    # 1.d Filter out any internal extra whitespace\n","    print(\"1.d Filter out extra whitespaces:\")\n","    cleaned_news_df = no_punct_news_df.select(trim(regexp_replace(col(column_name), \" +\", \" \")).alias(column_name))\n","    #cleaned_news_df.show(10)\n","\n","    # 2. Tokenization (split text into tokens)\n","    print(\"# 2. Tokenization:\")\n","    tokenizer = Tokenizer(inputCol=column_name, outputCol=\"tokens\")\n","    tokens_df = tokenizer.transform(cleaned_news_df).cache()\n","    #tokens_df.show(10)\n","\n","    # 3. Stopwords removal\n","    print(\"# 3. Stopwords removal:\")\n","    stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"terms\")\n","    terms_df = stopwords_remover.transform(tokens_df).cache()\n","    #terms_df.show(10)\n","\n","    # 4. Stemming (Snowball stemmer)\n","    print(\"# 4. Stemming:\")\n","    stemmer = SnowballStemmer(language=\"english\")\n","    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n","    terms_stemmed_df = terms_df.withColumn(\"terms_stemmed\", stemmer_udf(\"terms\")).cache()\n","    #terms_stemmed_df.show(10)\n","    \n","    return terms_stemmed_df\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-07681ca6B0"},"source":["# This function is responsible to implement the pipeline above for transforming categorical features into numerical ones\n","def to_numerical(df, numerical_features, categorical_features, target_variable):\n","\n","    \"\"\"\n","    Args:\n","        - df: the input dataframe\n","        - numerical_features: the list of column names in `df` corresponding to numerical features\n","        - categorical_features: the list of column names in `df` corresponding to categorical features\n","        - target_variable: the column name in `df` corresponding to the target variable\n","\n","    Return:\n","        - transformer: the pipeline of transformation fit to `df` (for future usage)\n","        - df_transformed: the dataframe transformed according to the pipeline\n","    \"\"\"\n","    \n","    from pyspark.ml import Pipeline\n","    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n","\n","\n","    # 1. Create a list of indexers, i.e., one for each categorical feature\n","    indexers = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c), handleInvalid=\"keep\") for c in categorical_features]\n","    \n","    \n","\n","    # 2. Create the one-hot encoder for the list of features just indexed (this encoder will keep any unseen label in the future)\n","    encoder = OneHotEncoder(inputCols=[indexer.getOutputCol() for indexer in indexers], \n","                                    outputCols=[\"{0}_encoded\".format(indexer.getOutputCol()) for indexer in indexers], \n","                                    handleInvalid=\"keep\")\n","\n","    # 3. Indexing the target column (i.e., transform it into 0/1) and rename it as \"label\"\n","    # Note that by default StringIndexer will assign the value `0` to the most frequent label, which in the case of `deposit` is `no`\n","    # As such, this nicely resembles the idea of having `deposit = 0` if no deposit is subscribed, or `deposit = 1` otherwise.\n","    label_indexer = StringIndexer(inputCol = target_variable, outputCol = \"label\")\n","    \n","    # 4. Assemble all the features (both one-hot-encoded categorical and numerical) into a single vector\n","    assembler = VectorAssembler(inputCols=encoder.getOutputCols() + numerical_features, outputCol=\"features\")\n","\n","    # 5. Populate the stages of the pipeline\n","    stages = indexers + [encoder] + [label_indexer] + [assembler]\n","\n","    # 6. Setup the pipeline with the stages above\n","    pipeline = Pipeline(stages=stages)\n","\n","    # 7. Transform the input dataframe accordingly\n","    transformer = pipeline.fit(df)\n","    df_transformed = transformer.transform(df)\n","\n","    # 8. Eventually, return both the transformed dataframe and the transformer object for future transformations\n","    return transformer, df_transformed "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KSvhLEQ-H81m"},"source":["### **Check null rows**\n","For our purpose, there must not be null values on any of the column. So we check that there aren't any null values in our dataset:"]},{"cell_type":"code","metadata":{"id":"-j2zQaFszWJF"},"source":["df.toPandas().isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQmaX9hRRfAh"},"source":["\n","Unfortunately, there are some rows with null values. We drop them in order to not have errors during the execution of some functions: \n"]},{"cell_type":"code","metadata":{"id":"z7Aa6rD8tidE"},"source":["df = df.na.drop()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ymHLG4v4RiFJ"},"source":["At one point of this Colab, we will use OneHotEncoding to transform String values in Numeric values to use them in our models. <br>OneHotEncoding needs values different from Blank String (e.g. \"\"), so we filter rows that haven't Blank String in any column."]},{"cell_type":"code","metadata":{"id":"eYPv3fVQ5pc3"},"source":["df = df.filter(df['title'] != \"\")\n","df = df.filter(df['description'] != \"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g-vLczwIuUhG"},"source":["# **3. Dataset Analysis**\n","The Dataset is available at this link:\n","\n","https://www.kaggle.com/kritanjalijain/amazon-reviews\n","\n","The first thing that we have to do is to load the dataset.\n","We create a variable called *nrows* to indicate how many rows of the dataset we want to process.\n","Due to the limited environment, we can load at most more or less 50.000 record at one time (if we try to use more than 50.000 elements, some models will fail returning \"Java heap space\").\n","If you want to process more than of 50.000, we suggest to use only one model at time (and, anyway, not more than 100.000 elements at time).\n"]},{"cell_type":"code","metadata":{"id":"3MpLDaiEzS_u","colab":{"base_uri":"https://localhost:8080/","height":323},"executionInfo":{"status":"error","timestamp":1624978153389,"user_tz":-120,"elapsed":2276,"user":{"displayName":"Atalla","photoUrl":"","userId":"01958578645143995824"}},"outputId":"1d316079-0c12-47a6-c027-7ded1c47400a"},"source":["spark = SparkSession.builder \\\n","    .master('local[*]') \\\n","    .config(\"spark.kryoserializer.buffer\", \"64m\") \\\n","    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n","    .appName('Amazon') \\\n","    .getOrCreate() \\\n","    "],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-d3925c8b8e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.kryoserializer.buffer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"64m\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.kryoserializer.buffer.max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1024m\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Amazon'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"DevlrMcPw1ZI"},"source":["## **Read dataset file into a Spark Dataframe**"]},{"cell_type":"code","metadata":{"id":"qKi5Hd60FFcX"},"source":["df = spark.read.load(\"/content/drive/MyDrive/MAGISTRALE/BigData/Project/Amazon/train.csv\", \n","                         format=\"csv\", \n","                         sep=\",\", \n","                         inferSchema=\"true\", \n","                         header=\"false\"\n","                         )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"moU-JQK_DVCH"},"source":["df = df.sample(withReplacement=False, fraction=0.014)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zw6YBOqhAOmu"},"source":["df.toPandas().to_csv('/content/drive/MyDrive/MAGISTRALE/BigData/Project/Amazon/random50k.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IPTd8ep9x74H"},"source":["## **Check the shape of the loaded dataset (Number of rows and column)**"]},{"cell_type":"code","metadata":{"id":"JyRyYqeXGA4l"},"source":["print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(df.count(), len(df.columns)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H2eo--0YEQsN"},"source":["## **Rename of the columns**\n","The dataset has no Column Name, so the very first thing that we have to do, after loading the dataset, is to give to our column a name.\n","By default, if there are no Column Name available in the imported Dataset, PySpark creates a dataframe with Column Names called <br><br>**_c{index}**<br><br>\n","To make it more Human Readable, we rename the columns as follow:<br>\n","\n","*   _c0 -> **classification**\n","*   _c1 -> **title**\n","*   _c2 -> **description**\n","\n"]},{"cell_type":"code","metadata":{"id":"mZCaQ3NLEQPB"},"source":["df = df.withColumnRenamed(\"_c0\", \"classification\").withColumnRenamed(\"_c1\", \"title\").withColumnRenamed(\"_c2\", \"description\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cojLTJibEbyL"},"source":["## **Check if the dataset is balanced**\n","One important characteristic is that the want our Dataset to be balanced, so we want to have more or less half of examples classified as **Positive** and half classified as **Negative**"]},{"cell_type":"code","metadata":{"id":"_magU3OjDDbP"},"source":["df.groupBy(\"classification\").count().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K3BawWPdQ90z"},"source":["So, the Dataset is balaced and we can proceed doing our analysis."]},{"cell_type":"markdown","metadata":{"id":"-WC4RPQgyEsB"},"source":["## **Print out the schema of the loaded dataset**\n","Just to give more information, our dataset has this schema"]},{"cell_type":"code","metadata":{"id":"Q3KhtSnvGIwG"},"source":["df.printSchema()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2JYf8W_QRHBc"},"source":["We remind that our models works with numerical value, so we have to distinguish which columns is not a number and process it in order to make it a numeric."]},{"cell_type":"code","metadata":{"id":"xsHSUHEzDS1-"},"source":["# Let's define some constants which we will use throughout this notebook\n","NUMERICAL_FEATURES = []\n","CATEGORICAL_FEATURES = ['title', 'description']\n","TARGET_VARIABLE = \"classification\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VOOQNlQzRmxu"},"source":["## **Text Cleaning**\n","We now use the function defined before to clean text and delete all not useful informations."]},{"cell_type":"code","metadata":{"id":"J7d2X29etAvJ"},"source":["print(\"Cleaning column title\")\n","title_cleaned_df = clean_text(df, \"title\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQdbwJHyuHcr"},"source":["print(\"Cleaning column description\")\n","description_cleaned_df = clean_text(df, \"description\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"76EPMVPXRrpc"},"source":["## **Correlating cleaned values with input rows**\n","After cleaning rows, we need to join the original rows with the cleaned ones.<br>\n","In order to do so, we added a column \"id\" to identify rows (on each Dataframe) and then join the Dataframes using this column. "]},{"cell_type":"code","metadata":{"id":"EsMeCt6-u1GS"},"source":["#aggiungiamo gli id delle righe per fare i join\n","clean_title_indexed = title_cleaned_df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n","clean_description_indexed = description_cleaned_df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())\n","initial_indexed = df.select(\"*\").withColumn(\"id\", monotonically_increasing_id())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1q7KBoqs0VM"},"source":["#rinomino le colonne del dataframe pulito, così quando effettuiamo il join capiamo quali colonne sono utili\n","clean_title_indexed = clean_title_indexed \\\n","                                        .withColumnRenamed(\"title\", \"_title_clean\") \\\n","                                        .withColumnRenamed(\"terms\", \"_title_terms\") \\\n","                                        .withColumnRenamed(\"tokens\", \"_title_tokens\") \\\n","                                        .withColumnRenamed(\"terms_stemmed\", \"_title_terms_stemmed\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ae90U9wixx34"},"source":["#rinomino le colonne del dataframe pulito, così quando effettuiamo il join capiamo quali colonne sono utili\n","clean_description_indexed = clean_description_indexed \\\n","                                                    .withColumnRenamed(\"description\", \"_description_clean\") \\\n","                                                    .withColumnRenamed(\"terms\", \"_description_terms\") \\\n","                                                    .withColumnRenamed(\"tokens\", \"_description_tokens\") \\\n","                                                    .withColumnRenamed(\"terms_stemmed\", \"_description_terms_stemmed\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VdTu_w7syCRb"},"source":["#Join di tutti e 3 i dataframe\n","mid_join_df = clean_title_indexed.join(clean_description_indexed, \"id\")\n","initial_indexed_only_target = initial_indexed.select(initial_indexed['id'], initial_indexed['classification'])\n","#Ultimo join con la tabella iniziale con la colonna target\n","final_df = mid_join_df.join(initial_indexed_only_target, \"id\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VzA5j_dURvKp"},"source":["## **Feature Selection**\n","At this point, we have a Dataframe composed in this way:"]},{"cell_type":"code","metadata":{"id":"YOOyQQ8sR3fe"},"source":["final_df.show(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UoIxuKU5R2Op"},"source":["We define two variables to use during the training phase.<br>Changing them will change the result of the training"]},{"cell_type":"code","metadata":{"id":"Kpa8s8G8Ezs5"},"source":["feature_1 = \"_title_terms\"\n","feature_2 = \"_description_terms\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lUzr4U740TYY"},"source":["final_df = final_df.select(final_df[\"classification\"],final_df[feature_1],final_df[feature_2])\n","final_df = final_df \\\n","                  .withColumnRenamed(\"classification\", \"classification\") \\\n","                  .withColumnRenamed(feature_1, \"title\") \\\n","                  .withColumnRenamed(feature_2, \"description\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k3JjypFGWX_U"},"source":["final_df = final_df \\\n","                  .withColumn(\"title\", concat_ws(\" \", \"title\")) \\\n","                  .withColumn(\"description\", concat_ws(\" \", \"description\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_j45_M_vWx53"},"source":["final_df = final_df.filter(final_df['title'] != \"\")\n","final_df = final_df.filter(final_df['description'] != \"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0pBUnmiLj7I"},"source":["## **Check that everything is OK**"]},{"cell_type":"code","metadata":{"id":"WVYcMVUAzrHE"},"source":["print(\"The shape of the dataset is {:d} rows by {:d} columns\".format(final_df.count(), len(final_df.columns)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U8i5YQYJf7AH"},"source":["print(\"Showing first 5 rows of final dataframe\")\n","final_df.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-fuY3fcW_bML"},"source":["# **4. Data Exploration**"]},{"cell_type":"code","metadata":{"id":"fVpjaEEW5rpm"},"source":["from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CI4aSrlI6LH7"},"source":["most_used_words_title = \" \".join(review for review in final_df.toPandas()['title'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUalMlOR5Ynz"},"source":["# Start with one review:\n","#text = final_df.toPandas()['title']\n","print(\"Showing most frequent words in title\")\n","# Create and generate a word cloud image:\n","wordcloud = WordCloud(width = 1000, height = 500).generate(most_used_words_title)\n","plt.figure(figsize=(15,8))\n","# Display the generated image:\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQMjJy0GIBDG"},"source":["most_used_words_description = \" \".join(review for review in final_df.toPandas()['description'])\n","# Start with one review:\n","#text = final_df.toPandas()['title']\n","print(\"Showing most frequent words in title\")\n","# Create and generate a word cloud image:\n","wordcloud = WordCloud(width = 1000, height = 500).generate(most_used_words_description)\n","plt.figure(figsize=(15,8))\n","# Display the generated image:\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JCY5UWnPInr2"},"source":["most_used_words_title_positive = \" \".join(review for review in final_df.filter(final_df['classification'] == 2).toPandas()['title'])\n","# Start with one review:\n","#text = final_df.toPandas()['title']\n","print(\"Showing most frequent words in POSITIVE title\")\n","# Create and generate a word cloud image:\n","wordcloud = WordCloud(width = 1000, height = 500).generate(most_used_words_title_positive)\n","plt.figure(figsize=(15,8))\n","# Display the generated image:\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OE7rQ0XJU1Y"},"source":["most_used_words_title_negative = \" \".join(review for review in final_df.filter(final_df['classification'] == 1).toPandas()['title'])\n","# Start with one review:\n","#text = final_df.toPandas()['title']\n","print(\"Showing most frequent words in NEGATIVE title\")\n","# Create and generate a word cloud image:\n","wordcloud = WordCloud(width = 1000, height = 500).generate(most_used_words_title_negative)\n","plt.figure(figsize=(15,8))\n","# Display the generated image:\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fFZ4szYNKUCc"},"source":["most_used_words_description_positive = \" \".join(review for review in final_df.filter(final_df['classification'] == 2).toPandas()['description'])\n","# Start with one review:\n","#text = final_df.toPandas()['title']\n","print(\"Showing most frequent words in POSITIVE title\")\n","# Create and generate a word cloud image:\n","wordcloud = WordCloud(width = 1000, height = 500).generate(most_used_words_description_positive)\n","plt.figure(figsize=(15,8))\n","# Display the generated image:\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JAKPbILKlF_"},"source":["most_used_words_description_negative = \" \".join(review for review in final_df.filter(final_df['classification'] == 1).toPandas()['description'])\n","# Start with one review:\n","#text = final_df.toPandas()['title']\n","print(\"Showing most frequent words in NEGATIVE title\")\n","# Create and generate a word cloud image:\n","wordcloud = WordCloud(width = 1000, height = 500).generate(most_used_words_description_negative)\n","plt.figure(figsize=(15,8))\n","# Display the generated image:\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"66pFGdch-BU_"},"source":["The Learning Pipeline**"]},{"cell_type":"markdown","metadata":{"id":"w4-GszAFNmSc"},"source":["### **Balanced vs. Unbalanced Dataset**\n","\n","So far, we haven't looked at how the binary target variable `deposit` is distributed across the instances of our dataset. In this \"lucky\" example, we know that _positive_ examples (i.e., instances where `deposit = 1`) and _negative_ examples (i.e., instances where `deposit = 0`) are somehow balanced (i.e., around 50% of the instances are positives and the other 50% are negatives). That is due to the way this sample dataset has been extracted from the original one.\n","\n","Most often, though, we have to deal with (very) unbalanced datasets where the minority class (which is usually the one we are interested in!) is accounting only for a small fraction of the total number of training instances. For example, consider the click-through rate (CTR) prediction problem, where we want to foresee whether an advertisement (or, in general, a web page) will be clicked by a user. There, most of the advertisements will not be clicked (negatives), whilst only a tiny fraction (even smaller than 1%) of them will be.\n","\n","The fact that a dataset is balanced (respectively, unbalanced) affects the process which we should use to correctly splitting it into _training_ and _test_ set. In particular:\n","\n","- If the dataset is (almost) balanced, we can safely use a **simple random sampling** strategy, which assigns to every instance the same probability of being selected (i.e., if there are _m_ instances, each one will be picked with the same uniform probability _p = 1/m_);\n","- If the dataset is (very) unbalanced, simple random sampling might lead to a poor splitting strategy, where - for instance - the test set ends up containing only examples that are labeled with the most representative class. To overcome such an issue, **stratified random sampling** is the right choice to take as it guarantees that both the training and the test split follow the same class distribution observed in the original dataset (e.g., if the dataset contains 99% of negative instances and 1% of positive ones, so will the training and the test set). This works by first \"stratifying\" the data according to the two groups (i.e., positives vs. negatives), and within each group apply simple random sampling. For example, if our original dataset contains _m_ instances so that _m_ = (_m+_) + (_m-_) and _m+ << _m- (e.g., _m+_/_m_ = 0.01) and we want to sample _k_ < _m_ instances out of the dataset, we will first stratify the original dataset and will select _k+_ = _km+_/_m_ positive instances and _k-_ = _km-_/_m_ negative instances, respectively."]},{"cell_type":"markdown","metadata":{"id":"pbBHU28yS36r"},"source":["### Let's first verify our dataset is actually _balanced_"]},{"cell_type":"code","metadata":{"id":"kF1A7XAsS911"},"source":["final_df.groupBy(TARGET_VARIABLE).count().show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jhyy7RTOZjSa"},"source":["# **5. Data Preparation**"]},{"cell_type":"markdown","metadata":{"id":"xlUz0-TNb4Jo"},"source":["### **Dataset Splitting: Training vs. Test Set**\n","\n","Before moving along with any preprocessing involving data transformations, we will split our dataset into **2** portions:\n","- _training set_ (e.g., accounting for **80%** of the total number of instances);\n","- _test set_ (e.g., accounting for the remaining **20%** of instances)"]},{"cell_type":"code","metadata":{"id":"1kmm1XjB2U4N"},"source":["RANDOM_SEED = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSzZLA9QcA_P"},"source":["# Randomly split our original dataset `house_df` into 80÷20 for training and test, respectively\n","train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=RANDOM_SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IRIWWJnJLo7g"},"source":["print(\"Training set size: {:d} instances\".format(train_df.count()))\n","print(\"Test set size: {:d} instances\".format(test_df.count()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"11HcGwvdg90A"},"source":["### **Transform Categorical features into Numerical using One-Hot Encoding**\n","\n","Note that this step is not always mandatory (e.g., decision trees are able to work nicely with categorical features without the need of transforming them to numerical). Still, other methods (like logistic regression) are designed to operate with numerical inputs only.\n","\n","To transform _categorical_ features into _numerical_ ones we proceed as follows.\n","We setup a pipeline which is composed of the following steps:\n","- [`StringIndexer`](https://spark.apache.org/docs/latest/ml-features#stringindexer): encodes a string column of labels to a column of label indices. The indices are in `[0, numLabels)`, and 4 ordering options are supported (default `frequencyDesc`, which assigns the most frequent label the index `0`, and so on and so forth).\n","- [`OneHotEncoderEstimator`](https://spark.apache.org/docs/latest/ml-features#onehotencoderestimator): maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. An important parameter is `handleInvalid`, which indicates how to deal with previously unseen labels. By default this raises an error but it can be set to as `keep` to assign previously unseen labels a fallback value.\n","- [`VectorAssembler`](https://spark.apache.org/docs/latest/ml-features#vectorassembler): is a transformer that combines a given list of columns into a single vector column."]},{"cell_type":"code","metadata":{"id":"ueRiKbiTxJsr"},"source":[" # Transform the training set and get back both the transformer and the new dataset\n","oh_transformer, oh_train_df = to_numerical(train_df, NUMERICAL_FEATURES, CATEGORICAL_FEATURES, TARGET_VARIABLE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vTix1SBg71Xr"},"source":["# Show the result of numerical transformation\n","#oh_train_df.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yYpzQRWqyxUx"},"source":["# Select `features` and `label` (i.e., formerly `deposit`) target variable only\n","train = oh_train_df.select([\"features\", \"label\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ena6EJ0Y5qhx"},"source":["## **Use the One-Hot encoding pipeline to transform the Test Set**"]},{"cell_type":"code","metadata":{"id":"K2pZg_Pey-yP"},"source":["#train.show(5, truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KPgeFLQC1S1t"},"source":["# Here, we use the same transformer as the one returned by the `to_numerical` function above yet applied to the test set\n","oh_test_df = oh_transformer.transform(test_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMNJ1NdtwXB8"},"source":["# Select `features` and `label` only\n","test = oh_test_df.select([\"features\", \"label\"])\n","#test.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"66mqwSUIUwss"},"source":["# **6. Training and Evaluation of Models**\n","\n","\n","*   **Logistic Regression**\n","*   **Naive Bayes Classifier**\n","*   **Support Vector Machine (SVM)**\n","*   **Gradient Boosted Tree Classifier(GBTC)**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"teSVf-WFzvs-"},"source":["## **Logistic Regression**\n","\n","In particular, we can specify the following parameters:\n","\n","- `regParam` is the regularization parameter (or $\\lambda$);\n","- `elasticNetParam` is the tradeoff parameter for regularization penalties (or $\\alpha$);\n","  - `regParam = 0` and `elasticNetParam = 0` means there is no regularization;\n","  - `regParam > 0` and `elasticNetParam = 0` means there is only L2-regularization; \n","  - `regParam > 0` and `elasticNetParam = 1` means there is only L1-regularization;\n","  - `regParam > 0` and `0 < elasticNetParam < 1` means there is both L1- and L2-regularization (Elastic Net);\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TITJHIgMXj1A"},"source":["### **Build and Train Logistic Regression Model** "]},{"cell_type":"code","metadata":{"id":"Mmgq2uMK0AMF"},"source":["from pyspark.ml.classification import LogisticRegression # This corresponds to LogisticRegressionWithLBFGS\n","\n","# This setting corresponds to no regularization at all (i.e., both regParam=0 and elasticNetParam=0)\n","log_reg = LogisticRegression(featuresCol = \"features\", labelCol = \"label\", maxIter=100, regParam=1, elasticNetParam=0.5)\n","log_reg_model = log_reg.fit(train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ame2dV8QZhSg"},"source":["### **Evaluate Performance on Training Data**"]},{"cell_type":"code","metadata":{"id":"COSfC7R0ZRO5"},"source":["lr_summary=log_reg_model.summary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lLnn2B6hZVRr"},"source":["lr_summary.accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TecoBuXHZYRl"},"source":["lr_summary.areaUnderROC"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PnbJw1beZcVA"},"source":["#lr_summary.weightedRecall"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ra8XUwLOZcON"},"source":["#r_summary.weightedPrecision"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LokzdSNP6T48"},"source":["### **Compute predictions on the Test Set according to the model learned on the Training Set**"]},{"cell_type":"code","metadata":{"id":"Y-tiLfEA1tml"},"source":["# `log_reg_model` is a Transformer which can be used to \"transform\" our test set\n","predictions = log_reg_model.transform(test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9fue7UUz5mom"},"source":["### **Evaluate model performance on the Test Set**"]},{"cell_type":"code","metadata":{"id":"0GySC6TS5c_i"},"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","evaluator = BinaryClassificationEvaluator(metricName= 'areaUnderROC')\n","lr_auroc = evaluator.evaluate(predictions)\n","print('Test Set AUC: {:.3f}'.format(lr_auroc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w-wYKo0Pm91z"},"source":["evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(predictions)\n","print ('Model Accuracy:{:.3f}'.format(accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dw2J1h81pi7N"},"source":["## **Naive Bayes Classifier**"]},{"cell_type":"markdown","metadata":{"id":"J96LLOFCp33u"},"source":["### **Build and Train Naive Bayes Classifier Model**"]},{"cell_type":"code","metadata":{"id":"JcSxHidDp0zY"},"source":["nb = NaiveBayes(featuresCol = \"features\", labelCol = \"label\", predictionCol= 'prediction')\n","nb_model = nb.fit(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mj_4Ggn0qZDt"},"source":["model_predictions = nb_model.transform(test)\n","#model_predictions.select(['label','probability', 'prediction']).show(10,False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WwL3YrgKqKxY"},"source":["### **Evaluate Performance on Test Data**"]},{"cell_type":"code","metadata":{"id":"z0TM1jf3pilR"},"source":["nb_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n","nb_auroc = nb_evaluator.evaluate(model_predictions)\n","print('The auc value of NB Classifier is {:.3f}'.format(nb_auroc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIEdWVDxwCFH"},"source":["nb_evaluator = BinaryClassificationEvaluator()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ytBF7jRZvqZH"},"source":["evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(model_predictions)\n","print ('Model Accuracy:{:.3f}'.format(accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKDttDPiVR4k"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y-9pyIP52i5h"},"source":["## **Support Vector Machine(SVM)**"]},{"cell_type":"markdown","metadata":{"id":"E2UkDKLNWhWv"},"source":["### **Build and Train SVM Model**"]},{"cell_type":"code","metadata":{"id":"1m2lc3SL2rZn"},"source":["lsvc = LinearSVC(featuresCol = \"features\", labelCol = \"label\", maxIter=100, predictionCol= 'prediction')\n","lsvc_model = lsvc.fit(train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rjah5qjSWuIY"},"source":["### **Evaluate Performance on Test Data**"]},{"cell_type":"code","metadata":{"id":"FrbML8Pa2vfG"},"source":["model_predictions = lsvc_model.transform(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rb3XFStA20Gu"},"source":["svc_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n","svc_auroc = svc_evaluator.evaluate(model_predictions)\n","print('The auc value of SupportVectorClassifier is {:.3f}'.format(svc_auroc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHP4KXB87s--"},"source":["evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(model_predictions)\n","print ('Model Accuracy:{:.3f}'.format(accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4OaSjRPWsrn"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"C1MAtiY3CWuc"},"source":["## **Gradient Boosted Tree Classifier (GBTC)**"]},{"cell_type":"code","metadata":{"id":"G_uiQepqCavR"},"source":["gbt = GBTClassifier()\n","gbt_model = gbt.fit(train)\n","model_predictions = gbt_model.transform(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTYIT6AnCmE_"},"source":["gbt_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n","gbt_auroc = gbt_evaluator.evaluate(model_predictions)\n","print('The auc value of GradientBoostedTreesClassifier is {:.3f}'.format(gbt_auroc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-YxjgdgCwf8"},"source":["evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(model_predictions)\n","print ('Model Accuracy:{:.3f}'.format(accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_phBLqqZ8dzm"},"source":["### Random Forest libro"]},{"cell_type":"code","metadata":{"id":"QZPV3ARS8lI6"},"source":["from pyspark.ml.classification import RandomForestClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9a9UN6Z8txe"},"source":["rf = RandomForestClassifier(numTrees=50,maxDepth=30)\n","rf_model = rf.fit(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b3HMyOkT8yEq"},"source":[" model_predictions=rf_model.transform(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w0jFXFCr8-EO"},"source":["rf_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n","rf_auroc = rf_evaluator.evaluate(model_predictions)\n","print(f'The auc value of RandomForestClassifier Model is {rf_auroc}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EjpEQHcFEsI1"},"source":["evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"confu\")\n","accuracy = evaluator.evaluate(model_predictions)\n","print ('Model Accuracy:{:.3f}'.format(accuracy))"],"execution_count":null,"outputs":[]}]}